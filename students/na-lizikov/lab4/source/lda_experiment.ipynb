{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №4: Латентное размещение Дирихле (LDA)\n",
    "\n",
    "## Загрузка и предобработка данных (20 Newsgroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер корпуса: (18846, 2000)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Загрузка датасета\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "texts = newsgroups.data\n",
    "\n",
    "# Предобработка\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z ]', ' ', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "texts_clean = [preprocess(doc) for doc in texts]\n",
    "\n",
    "# Преобразование в мешок слов\n",
    "vectorizer = CountVectorizer(max_features=2000)\n",
    "X = vectorizer.fit_transform(texts_clean)\n",
    "\n",
    "print(f'Размер корпуса: {X.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение и анализ: Ручная реализация LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Итерация 10/100 завершена\n",
      "Итерация 20/100 завершена\n",
      "Итерация 30/100 завершена\n",
      "Итерация 40/100 завершена\n",
      "Итерация 50/100 завершена\n",
      "Итерация 60/100 завершена\n",
      "Итерация 70/100 завершена\n",
      "Итерация 80/100 завершена\n",
      "Итерация 90/100 завершена\n",
      "Итерация 100/100 завершена\n",
      "Тема 1: drive, card, do, system, disk, thanks, would, use, window, know\n",
      "Тема 2: key, government, president, system, public, use, information, chip, state, number\n",
      "Тема 3: god, jesus, one, christian, say, believe, christ, bible, man, belief\n",
      "Тема 4: armenian, people, gun, state, said, government, israel, right, child, war\n",
      "Тема 5: max, space, nasa, earth, disease, system, bhj, patient, medical, stephanopoulos\n",
      "Тема 6: one, would, get, like, time, car, know, good, back, could\n",
      "Тема 7: would, people, think, one, like, know, get, make, right, say\n",
      "Тема 8: one, book, would, people, group, church, article, word, also, time\n",
      "Тема 9: file, edu, image, window, program, use, com, ftp, available, version\n",
      "Тема 10: game, team, year, player, play, win, season, last, first, hockey\n",
      "Время обучения (ручная реализация): 2528.24 секунд\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from lda_manual import LDAManual\n",
    "\n",
    "n_topics = 10\n",
    "n_iter = 100\n",
    "\n",
    "lda_manual = LDAManual(n_topics=n_topics, n_iter=n_iter, alpha=0.1, beta=0.01, random_state=42)\n",
    "\n",
    "start = time.time()\n",
    "lda_manual.fit(X)\n",
    "manual_time = time.time() - start\n",
    "\n",
    "# Получаем топ-10 слов для каждой темы\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "top_words_manual = lda_manual.get_top_words(feature_names, n_top_words=10)\n",
    "\n",
    "for idx, words in enumerate(top_words_manual):\n",
    "    print(f'Тема {idx+1}:', ', '.join(words))\n",
    "\n",
    "print(f'Время обучения (ручная реализация): {manual_time:.2f} секунд')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение и анализ: LDA из sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тема 1: window, drive, do, card, system, disk, problem, use, bit, work\n",
      "Тема 2: space, program, please, thanks, would, information, anyone, know, application, window\n",
      "Тема 3: armenian, state, year, new, people, war, muslim, turkish, russian, american\n",
      "Тема 4: god, game, one, jesus, christian, team, would, year, church, player\n",
      "Тема 5: would, people, key, government, law, right, gun, think, one, know\n",
      "Тема 6: max, israel, israeli, arab, bhj, giz, jew, medical, disease, palestinian\n",
      "Тема 7: one, would, people, think, may, like, say, many, thing, make\n",
      "Тема 8: one, get, like, time, would, know, back, said, say, could\n",
      "Тема 9: car, new, price, one, like, good, would, sale, get, also\n",
      "Тема 10: file, edu, image, program, com, available, ftp, version, use, graphic\n",
      "Время обучения (sklearn): 47.10 секунд\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_sklearn = LatentDirichletAllocation(n_components=n_topics, max_iter=10, learning_method='batch', random_state=42)\n",
    "\n",
    "start = time.time()\n",
    "lda_sklearn.fit(X)\n",
    "sklearn_time = time.time() - start\n",
    "\n",
    "# Получаем топ-10 слов для каждой темы\n",
    "top_words_sklearn = []\n",
    "for topic_idx, topic in enumerate(lda_sklearn.components_):\n",
    "    top = topic.argsort()[::-1][:10]\n",
    "    top_words_sklearn.append([feature_names[i] for i in top])\n",
    "\n",
    "for idx, words in enumerate(top_words_sklearn):\n",
    "    print(f'Тема {idx+1}:', ', '.join(words))\n",
    "\n",
    "print(f'Время обучения (sklearn): {sklearn_time:.2f} секунд')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка когерентности тем и сравнение результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Когерентность (ручная реализация): 0.5897\n",
      "Когерентность (sklearn): 0.5307\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Преобразуем тексты в список токенов\n",
    "texts_tokens = [doc.split() for doc in texts_clean]\n",
    "dictionary = Dictionary(texts_tokens)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts_tokens]\n",
    "\n",
    "manual_topics = top_words_manual\n",
    "cm_manual = CoherenceModel(topics=manual_topics, texts=texts_tokens, dictionary=dictionary, coherence='c_v')\n",
    "coh_manual = cm_manual.get_coherence()\n",
    "print(f'Когерентность (ручная реализация): {coh_manual:.4f}')\n",
    "\n",
    "sklearn_topics = top_words_sklearn\n",
    "cm_sklearn = CoherenceModel(topics=sklearn_topics, texts=texts_tokens, dictionary=dictionary, coherence='c_v')\n",
    "coh_sklearn = cm_sklearn.get_coherence()\n",
    "print(f'Когерентность (sklearn): {coh_sklearn:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
