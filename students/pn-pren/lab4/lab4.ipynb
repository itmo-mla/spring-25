{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "579e2ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8361554",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87ba538",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLDA:\n",
    "    def __init__(self, n_topics, n_iter=100, alpha=0.1, beta=0.01):\n",
    "        self.n_topics = n_topics\n",
    "        self.n_iter = n_iter\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def fit(self, X):\n",
    "        n_docs, n_words = X.shape\n",
    "        self.phi = np.random.dirichlet([self.beta] * n_words, self.n_topics)\n",
    "        self.theta = np.random.dirichlet([self.alpha] * self.n_topics, n_docs)\n",
    "\n",
    "        for it in range(self.n_iter):\n",
    "            print(f\"Iteration {it+1}/{self.n_iter}\")\n",
    "\n",
    "            # E-шаг\n",
    "            p_tdw = np.zeros((n_docs, n_words, self.n_topics))\n",
    "            for d in range(n_docs):\n",
    "                for w in X[d].nonzero()[1]:\n",
    "                    probs = self.phi[:, w] * self.theta[d, :]\n",
    "                    p_tdw[d, w, :] = probs / probs.sum()\n",
    "\n",
    "            # M-шаг\n",
    "            nwt = np.zeros((self.n_topics, n_words))\n",
    "            ntd = np.zeros((n_docs, self.n_topics))\n",
    "            for d in range(n_docs):\n",
    "                for w in X[d].nonzero()[1]:\n",
    "                    count = X[d, w]\n",
    "                    for t in range(self.n_topics):\n",
    "                        p = p_tdw[d, w, t]\n",
    "                        nwt[t, w] += count * p\n",
    "                        ntd[d, t] += count * p\n",
    "\n",
    "            # обновление\n",
    "            self.phi = (nwt + self.beta - 1)\n",
    "            self.phi /= self.phi.sum(axis=1, keepdims=True)\n",
    "\n",
    "            self.theta = (ntd + self.alpha - 1)\n",
    "            self.theta /= self.theta.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def get_topics(self, vocab, n_top_words=10):\n",
    "        topics = []\n",
    "        for t in range(self.n_topics):\n",
    "            top_indices = self.phi[t].argsort()[-n_top_words:][::-1]\n",
    "            topics.append([vocab[i] for i in top_indices])\n",
    "        return topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)  #emails\n",
    "    text = re.sub(r'\\s+', ' ', text)  #newline chars\n",
    "    text = re.sub(r\"\\'\", \"\", text)  #single quotes\n",
    "    text = re.sub(r'[^\\w\\s]', '', text, re.UNICODE) #punctuation\n",
    "    return text.lower()\n",
    "\n",
    "def calculate_coherence(topics, texts, dictionary):\n",
    "    coherence_model = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    return coherence_model.get_coherence()\n",
    "\n",
    "def print_topics(topics):\n",
    "    for i, topic in enumerate(topics):\n",
    "        print(f\"Тема {i+1}: {' '.join(topic)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "961463f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/120\n",
      "Iteration 2/120\n",
      "Iteration 3/120\n",
      "Iteration 4/120\n",
      "Iteration 5/120\n",
      "Iteration 6/120\n",
      "Iteration 7/120\n",
      "Iteration 8/120\n",
      "Iteration 9/120\n",
      "Iteration 10/120\n",
      "Iteration 11/120\n",
      "Iteration 12/120\n",
      "Iteration 13/120\n",
      "Iteration 14/120\n",
      "Iteration 15/120\n",
      "Iteration 16/120\n",
      "Iteration 17/120\n",
      "Iteration 18/120\n",
      "Iteration 19/120\n",
      "Iteration 20/120\n",
      "Iteration 21/120\n",
      "Iteration 22/120\n",
      "Iteration 23/120\n",
      "Iteration 24/120\n",
      "Iteration 25/120\n",
      "Iteration 26/120\n",
      "Iteration 27/120\n",
      "Iteration 28/120\n",
      "Iteration 29/120\n",
      "Iteration 30/120\n",
      "Iteration 31/120\n",
      "Iteration 32/120\n",
      "Iteration 33/120\n",
      "Iteration 34/120\n",
      "Iteration 35/120\n",
      "Iteration 36/120\n",
      "Iteration 37/120\n",
      "Iteration 38/120\n",
      "Iteration 39/120\n",
      "Iteration 40/120\n",
      "Iteration 41/120\n",
      "Iteration 42/120\n",
      "Iteration 43/120\n",
      "Iteration 44/120\n",
      "Iteration 45/120\n",
      "Iteration 46/120\n",
      "Iteration 47/120\n",
      "Iteration 48/120\n",
      "Iteration 49/120\n",
      "Iteration 50/120\n",
      "Iteration 51/120\n",
      "Iteration 52/120\n",
      "Iteration 53/120\n",
      "Iteration 54/120\n",
      "Iteration 55/120\n",
      "Iteration 56/120\n",
      "Iteration 57/120\n",
      "Iteration 58/120\n",
      "Iteration 59/120\n",
      "Iteration 60/120\n",
      "Iteration 61/120\n",
      "Iteration 62/120\n",
      "Iteration 63/120\n",
      "Iteration 64/120\n",
      "Iteration 65/120\n",
      "Iteration 66/120\n",
      "Iteration 67/120\n",
      "Iteration 68/120\n",
      "Iteration 69/120\n",
      "Iteration 70/120\n",
      "Iteration 71/120\n",
      "Iteration 72/120\n",
      "Iteration 73/120\n",
      "Iteration 74/120\n",
      "Iteration 75/120\n",
      "Iteration 76/120\n",
      "Iteration 77/120\n",
      "Iteration 78/120\n",
      "Iteration 79/120\n",
      "Iteration 80/120\n",
      "Iteration 81/120\n",
      "Iteration 82/120\n",
      "Iteration 83/120\n",
      "Iteration 84/120\n",
      "Iteration 85/120\n",
      "Iteration 86/120\n",
      "Iteration 87/120\n",
      "Iteration 88/120\n",
      "Iteration 89/120\n",
      "Iteration 90/120\n",
      "Iteration 91/120\n",
      "Iteration 92/120\n",
      "Iteration 93/120\n",
      "Iteration 94/120\n",
      "Iteration 95/120\n",
      "Iteration 96/120\n",
      "Iteration 97/120\n",
      "Iteration 98/120\n",
      "Iteration 99/120\n",
      "Iteration 100/120\n",
      "Iteration 101/120\n",
      "Iteration 102/120\n",
      "Iteration 103/120\n",
      "Iteration 104/120\n",
      "Iteration 105/120\n",
      "Iteration 106/120\n",
      "Iteration 107/120\n",
      "Iteration 108/120\n",
      "Iteration 109/120\n",
      "Iteration 110/120\n",
      "Iteration 111/120\n",
      "Iteration 112/120\n",
      "Iteration 113/120\n",
      "Iteration 114/120\n",
      "Iteration 115/120\n",
      "Iteration 116/120\n",
      "Iteration 117/120\n",
      "Iteration 118/120\n",
      "Iteration 119/120\n",
      "Iteration 120/120\n"
     ]
    }
   ],
   "source": [
    "N_TOPICS = 4\n",
    "N_ITERATIONS = 120\n",
    "N_TOP_WORDS = 10\n",
    "MAX_FEATURES = 1000\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "newsgroups_train = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "processed_docs = [preprocess_text(doc) for doc in newsgroups_train.data]\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=MAX_FEATURES, stop_words=stop_words)\n",
    "X = vectorizer.fit_transform(processed_docs)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "texts_for_coherence = [[word for word in doc.split() if word in vocab] for doc in processed_docs]\n",
    "dictionary = Dictionary(texts_for_coherence)\n",
    "\n",
    "my_lda = MyLDA(n_topics=N_TOPICS, n_iter=N_ITERATIONS, alpha=0.4, beta=0.1)\n",
    "\n",
    "start_time = time.time()\n",
    "my_lda.fit(X)\n",
    "my_lda_time = time.time() - start_time\n",
    "\n",
    "my_lda_topics = my_lda.get_topics(vocab, n_top_words=N_TOP_WORDS)\n",
    "my_lda_coherence = calculate_coherence(my_lda_topics, texts_for_coherence, dictionary)\n",
    "\n",
    "sklearn_lda = LatentDirichletAllocation(n_components=N_TOPICS, max_iter=N_ITERATIONS,\n",
    "                                        learning_method='batch', random_state=42)\n",
    "start_time = time.time()\n",
    "sklearn_lda.fit(X)\n",
    "sklearn_lda_time = time.time() - start_time\n",
    "\n",
    "sklearn_lda_topics = []\n",
    "for topic_idx, topic in enumerate(sklearn_lda.components_):\n",
    "    top_words_indices = topic.argsort()[:-N_TOP_WORDS - 1:-1]\n",
    "    topic_words = [vocab[i] for i in top_words_indices]\n",
    "    sklearn_lda_topics.append(topic_words)\n",
    "sklearn_lda_coherence = calculate_coherence(sklearn_lda_topics, texts_for_coherence, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4e53664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Кастом LDA\n",
      "- Время обучени: 558.95 сек\n",
      "- Когерентность: 0.2811\n",
      "- Темы:\n",
      "  - Тема 1: explain baptism death study hate time anyone get said problems\n",
      "  - Тема 2: explain support per anyone made people posted get problems death\n",
      "  - Тема 3: support per anyone posted described faq provide control database following\n",
      "  - Тема 4: explain course made support people guess cant problems issue total\n",
      "\n",
      "scikit-learn LDA\n",
      "- Время обучени: 113.53 сек\n",
      "- Когерентность: 0.6927\n",
      "- Темы:\n",
      "  - Тема 1: god one would people jesus believe us church say bible\n",
      "  - Тема 2: image graphics jpeg file images available data software files also\n",
      "  - Тема 3: would one dont know like im get think people could\n",
      "  - Тема 4: medical health 10 disease patients cancer research 1993 information hiv\n",
      "\n",
      "Когерентность mylda: 0.2811\n",
      "Когерентность sklearn: 0.6927\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_summary = f\"\"\"\n",
    "Кастом LDA\n",
    "- Время обучени: {my_lda_time:.2f} сек\n",
    "- Когерентность: {my_lda_coherence:.4f}\n",
    "- Темы:\n",
    "\"\"\"\n",
    "for i, topic in enumerate(my_lda_topics):\n",
    "    results_summary += f\"  - Тема {i+1}: {' '.join(topic)}\\n\"\n",
    "\n",
    "results_summary += f\"\"\"\n",
    "scikit-learn LDA\n",
    "- Время обучени: {sklearn_lda_time:.2f} сек\n",
    "- Когерентность: {sklearn_lda_coherence:.4f}\n",
    "- Темы:\n",
    "\"\"\"\n",
    "for i, topic in enumerate(sklearn_lda_topics):\n",
    "    results_summary += f\"  - Тема {i+1}: {' '.join(topic)}\\n\"\n",
    "comparison_summary = f\"\"\"\\\n",
    "Когерентность mylda: {my_lda_coherence:.4f}\\n\\\n",
    "Когерентность sklearn: {sklearn_lda_coherence:.4f}\\n\n",
    "\"\"\"\n",
    "\n",
    "print(results_summary)\n",
    "print(comparison_summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_kernel",
   "language": "python",
   "name": "jupyter_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
