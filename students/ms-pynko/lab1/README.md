# Лабораторная работа №1. Ансамбли моделей

## Описание проекта
В этой работе реализован и исследован один из ансамблевых методов — **бэггинг** на базе решающих деревьев — и проведено сравнение с:  
1. Одиночным `DecisionTreeClassifier`  
2. Библиотечной реализацией `RandomForestClassifier` из `scikit-learn`

В качестве данных используется **Glass Classification** датасет.

## Описание метода
**Бэггинг** (Bootstrap AGGregatING) — метод повышения стабильности и точности моделей путём обучения нескольких базовых алгоритмов на разных бутстрэп-подвыборках исходных данных.
- **Bootstrap-подвыборки**: для каждого из \(n\_estimators\) деревьев создаётся выборка из \(m\) объектов (с возвращением).
- **Random Subspace**: при каждом разбиении дерева выбирается случайное подмножество признаков (обычно \(\sqrt{p}\), где \(p\) — число признаков).
- **Агрегация**: прогнозы объединяются **мажоритарным голосованием**.

## Описание датасета
**Glass Classification**:
- **Источник**: UCI ML Repository  
- **Записи**: 214  
- **Признаки (9)**: RI, Na, Mg, Al, Si, K, Ca, Ba, Fe  
- **Классы (6)**: типы стекла  
- **Кодирование меток**: `LabelEncoder`

## Результаты
```
=== Hand-made Bagging ===
Accuracy hand method: 0.7815
Recall hand method: 0.5617
Precession hand method: 0.5626
0.17833614349365234

=== Single Decision Tree ===
Single tree accuracy: 0.7037
Single tree recall: 0.6233
Single tree precession: 0.6660
0.004000425338745117

=== sklearn RandomForest ===
Sklearn accuracy: 0.7778
Sklearn recall: 0.6399
Sklearn precession: 0.6708
0.11798930168151855
```

---

## Выводы
- **Ручной бэггинг** при текущей реализации (с OOB-фильтрацией и клонированием деревьев) показывает точность (≈0.78). 
- **RandomForest** из sklearn даёт лучшие метрики (≈0.78), сохраняя разумное время обучения.  
- **DecisionTree** — самый быстрый (0.004 с), но уступает по качеству ансамблям.

